
## Quantifying XAI's Heatmaps
This project folder contains all the codes related to the paper [**Quantifying Explainability of Saliency Methods in Deep Neural Networks**](https://ieeexplore.ieee.org/abstract/document/9983502) published in IEEE TAI. Supp. material is here (https://www.researchgate.net/publication/360439499_xaibasiccyb_supppdf)

**Objective**: to provide a more objective way to evaluate heatmaps generated by eXplainable artificial intelligence (XAI) methods. A nice [tutorial by pytorch Captum](https://captum.ai/tutorials/Resnet_TorchVision_Interpret) demonstrates how the heatmap generation methods can be used. 

This framework is intended to provide unambiguous scoring of heatmap, quantitatively measuring their quality as the explanations to model predictions. The project introduces 3 main things:
* A **10-class synthetic cells** dataset. The dataset consists of images of 10 different types of cells (one of them is blank) placed in 3 different types of background. They are synthetic, and thus can be generated on demand. See fig. 1. The heatmaps consist of dark red regions (value=0.9), light red region (value=0.4) and white region (value=0) that delineate discriminative features, localization and non-feature respectively. 
* A metric defined to quantify heatmaps' values. We define the *five-band score* and makes use of a stratification process to accompany it. Depending on the context, different scores may be needed, but we hope eXplainable artificial intelligence (XAI) will grow with a good spirit of objectivity, by developing good methods of quantification similar to the way introduced in the above-mentioned paper.
* In version 2.0, the entire code base has been streamlined. Furthermore, we introduce mabCAM, a method to generate heatmaps by finetuning the model w.r.t magnitude-banded heatmaps.


![](https://drive.google.com/uc?export=view&id=1GjHAn62ahfeBOaRoxcVMOwpMuQP7nFN2)
Fig. 1. Sample images from all 10 classes in 3 different types of background (row 1-3). The corresponding heatmaps are in row 4-6.


Existing results are available in the google drive <a href="https://drive.google.com/drive/folders/1H6XfJkdDj-V_T3hnHZZFyoiKoOam3ttM?usp=share_link">link</a>.

Link to appendix files can be found in <a href="https://www.researchgate.net/publication/360439499_xaibasiccyb_supppdf">here</a>.


## Version 2.0

We experiment with magnitude-banded class activation mapping (mabCAM) and spatial product attention (SPA) on our synthetic dataset. 

Our results can be found in the following <a href="https://drive.google.com/drive/folders/1ap0lYjWf2BbiaLZuu1vTQz4NAvvijePw?usp=share_link">here</a>

The exact codes we use to run the entire experiment can be found in misc/expt_commands.txt.

![](https://drive.google.com/uc?export=view&id=1CcwX-pVVE-SzyGNYymC-MzDx8YuX0nGd)
Fig. 2. mabCAM. SPA consists of simple deconvolutional layers (green) finetuned to produce heatmaps compatible with our framework.


### Installation
For reproducibility, we use conda environment (env.yml provided). More installations are to be performed manually depending on your machine and cuda versions. Here is our pytorch installation.

```
pip3 install torch torchvision torchaudio --extra-index-url https://download.pytorch.org/whl/cu117
```
which is 1.13.0+cu117 at the time of installation. We use cuda V11.7.99.

To use the conda environment in jupyter notebook (e.g. for tutorials), 
```
python -m ipykernel install --user --name=xai_basic
```


### Main processes
*1. Data preparation*. The following are the steps used to reproduce our results. They show the formulas
```
python main.py --mode prepare_data --n_classes 3 --n_shards 16 --n_per_shard 256 --PROJECT_NAME project01
python main.py --mode prepare_data --n_classes 3 --n_shards 4 --n_per_shard 256 --PROJECT_NAME project01 --data_mode val
python main.py --mode prepare_data --n_classes 3 --n_shards 4 --n_per_shard 256 --PROJECT_NAME project01 --data_mode test
```

*2. Training and validation*
Replace R with the regularization value e.g. 1,5 or 10 and replace k with integer 1,2,3,... to label the k-th experiment
```
python main.py --mode train_val --PROJECT_NAME project01 --n_epoch 128 --loss CE+mab --mab_regularization R --model_name mabmodel_R_k 
```
To set minimum no of epoch, use --min_epoch 12.

*3. Evaluation* . This part runs mabCAM evaluation, along which soft five-band score results on different XAI methods are performed. It may take a long time.
```
python main.py --mode eval --PROJECT_NAME project01 --model_name mabmodel_10_1
```

### Visualization
The following will plot the recall and precision plots, and then evaluate accuracy results on test data and save heatmap images generated by mabCAM, grouping them based on correct and wrong class predictions. 
```
python main.py --mode visualization --PROJECT_NAME project01 --model_names mabmodel_10_1 mabmodel_10_2
python main.py --mode eval_and_gallery --PROJECT_NAME project01 --model_name mabmodel_10_1
```

## Version 1.0
All codes for the first iteration of this work have been moved to legacy/srcV1 folder. 

Codes are in python, Deep learning models in pytorch.

Be sure to check out the tutorial folder to see how to use or display the dataset.

The main results in the paper are produced using 
```
python main.py --mode workflow --mode2 workflow1
python main.py --mode workflow --mode2 workflow2
python main.py --mode workflow --mode2 workflow3
```
Use python main.py to explore any other commands.
